{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-your-first-pipeline\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'applicationInsights': '/subscriptions/165934ca-a3be-4df3-842b-332997fcecd9/resourcegroups/myresourcegroup/providers/microsoft.insights/components/llachmanmlwork7726509491',\n",
       " 'containerRegistry': '/subscriptions/165934ca-a3be-4df3-842b-332997fcecd9/resourceGroups/MyResourceGroup/providers/Microsoft.ContainerRegistry/registries/llachmanmlwo5de1ae17',\n",
       " 'creationTime': '2019-08-10T02:16:58.7668266+00:00',\n",
       " 'description': '',\n",
       " 'friendlyName': '',\n",
       " 'id': '/subscriptions/165934ca-a3be-4df3-842b-332997fcecd9/resourceGroups/MyResourceGroup/providers/Microsoft.MachineLearningServices/workspaces/llachmanMLWorkspace',\n",
       " 'identityPrincipalId': '2166891c-3629-4f17-8f8e-72c1b4d8d9fb',\n",
       " 'identityTenantId': 'c7970e8d-6d07-4b20-bf16-eba4864e56ef',\n",
       " 'identityType': 'SystemAssigned',\n",
       " 'keyVault': '/subscriptions/165934ca-a3be-4df3-842b-332997fcecd9/resourcegroups/myresourcegroup/providers/microsoft.keyvault/vaults/llachmanmlwork6145098727',\n",
       " 'location': 'centralus',\n",
       " 'name': 'llachmanMLWorkspace',\n",
       " 'storageAccount': '/subscriptions/165934ca-a3be-4df3-842b-332997fcecd9/resourcegroups/myresourcegroup/providers/microsoft.storage/storageaccounts/llachmanmlwork2978954949',\n",
       " 'tags': {},\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'workspaceid': 'cc72a18c-90b7-444d-b592-8a2d1f1272fb'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llachmanvm1': DsvmCompute(workspace=Workspace.create(name='llachmanmlworkspace', subscription_id='165934ca-a3be-4df3-842b-332997fcecd9', resource_group='myresourcegroup'), name=llachmanvm1, id=/subscriptions/165934ca-a3be-4df3-842b-332997fcecd9/resourceGroups/myresourcegroup/providers/Microsoft.MachineLearningServices/workspaces/llachmanmlworkspace/computes/llachmanvm1, type=VirtualMachine, provisioning_state=Succeeded, location=centralus, tags=None), 'cpucluster': AmlCompute(workspace=Workspace.create(name='llachmanmlworkspace', subscription_id='165934ca-a3be-4df3-842b-332997fcecd9', resource_group='myresourcegroup'), name=cpucluster, id=/subscriptions/165934ca-a3be-4df3-842b-332997fcecd9/resourceGroups/myresourcegroup/providers/Microsoft.MachineLearningServices/workspaces/llachmanmlworkspace/computes/cpucluster, type=AmlCompute, provisioning_state=Succeeded, location=centralus, tags=None)}\n"
     ]
    }
   ],
   "source": [
    "print(ws.compute_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default datastore - stores the data for the pipeline to access\n",
    "def_data_store = ws.get_default_datastore()\n",
    "\n",
    "# Get the blob storage associated with the workspace\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "\n",
    "# Get file storage associated with the workspace\n",
    "def_file_store = Datastore(ws, \"workspacefilestore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./data/20news.pkl\n",
      "Uploaded ./data/20news.pkl, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_e97f72386896406eb043b019334ffeee"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_blob_store.upload_files(\n",
    "    [\"./data/20news.pkl\"],\n",
    "    target_path=\"20newsgroups\",\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# A data source in a pipeline is represented by a DataReference object, \n",
    "# which points to data that lives in or is accessible from a datastore\n",
    "blob_input_data = DataReference(\n",
    "    datastore=def_blob_store,\n",
    "    data_reference_name=\"test_data\",\n",
    "    path_on_datastore=\"20newsgroups/20news.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate data is represented by a PipelineData object, and introduces a data dependency between steps\n",
    "output_data1 = PipelineData(\n",
    "    \"output_data1\",\n",
    "    datastore=def_blob_store,\n",
    "    output_name=\"output_data1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found compute target: cpucluster\n"
     ]
    }
   ],
   "source": [
    "# Ensure a compute_target is attached to our ML Workspace\n",
    "compute_name = \"cpucluster\"\n",
    "vm_size = \"STANDARD_D2_V2\"\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found compute target: ' + compute_name)\n",
    "    else:\n",
    "        print('Could not find compute target', compute_target, type(compute_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = 'project'\n",
    "\n",
    "# PythonScriptStep is a built-in pipeline step, which runs a Python script in a specified compute target\n",
    "trainStep = PythonScriptStep(\n",
    "    script_name=\"train.py\",\n",
    "    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n",
    "    inputs=[blob_input_data],\n",
    "    outputs=[output_data1],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=project_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extractStep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-baf6a830957c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# list of steps to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcompareModels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrainStep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractStep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompareStep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Build the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipeline1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompareModels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extractStep' is not defined"
     ]
    }
   ],
   "source": [
    "# list of steps to run\n",
    "compareModels = [trainStep, extractStep, compareStep]\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[compareModels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbStep = NotebooVMStep(\n",
    "    name=compute_name,\n",
    "    inputs=[step_1_input],\n",
    "    outputs=[step_1_output],\n",
    "    num_workers=1,\n",
    "    notebook_path=notebook_path,\n",
    "    notebook_params={'myparam': 'testparam'},\n",
    "    run_name='demo run name',\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=False\n",
    ")\n",
    "# List of steps to run\n",
    "steps = [dbStep]\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline1 = Pipeline(workspace=ws, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the pipeline to be run\n",
    "pipeline_run1 = Experiment(ws, 'Compare_Models_Exp').submit(pipeline1)\n",
    "pipeline_run1.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the pipeline before publishing so you can run it with different inputs later\n",
    "pipeline_param = PipelineParameter(\n",
    "  name=\"pipeline_arg\",\n",
    "  default_value=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this PipelineParameter object as a parameter to any of the steps in the pipeline as follows:\n",
    "compareStep = PythonScriptStep(\n",
    "  script_name=\"compare.py\",\n",
    "  arguments=[\"--comp_data1\", comp_data1, \"--comp_data2\", comp_data2, \"--output_data\", out_data3, \"--param1\", pipeline_param],\n",
    "  inputs=[ comp_data1, comp_data2],\n",
    "  outputs=[out_data3],\n",
    "  target=compute_target,\n",
    "  source_directory=project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish this pipeline that will accept a parameter when invoked\n",
    "published_pipeline1 = pipeline_run1.publish_pipeline(\n",
    "     name=\"My_Published_Pipeline\",\n",
    "     description=\"My Published Pipeline Description\",\n",
    "     version=\"1.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
